# Time Series Foundation Model
This repo focus on progress on Time Series Foundation Model.

- [Time Series Foundation Model](#time-series-foundation-model)
  - [Survey\&Benchmark](#surveybenchmark)
    - [2024](#2024)
  - [Work](#work)
    - [2023](#2023)
    - [2024](#2024-1)
  - [Dataset](#dataset)


## Survey&Benchmark

### 2024

- **A Survey of Deep Learning and Foundation Models for Time Series Forecasting.** *Miller(University of Georgia), John A., Mohammed Aldosari, Farah Saeed, Nasid Habib Barna, Subas Rana, I. Budak Arpinar, and Ninghao Liu.* [link](https://arxiv.org/abs/2401.13912)
- **Foundation Models for Time Series Analysis: A Tutorial and Survey.** *Yuxuan Liang(The Hong Kong University of Science and Technology(Guangzhou), Haomin Wen(Beijing Jiaotong University))* [link](https://arxiv.org/pdf/2403.14735)

## Work

### 2023

- **Large Language Models Are Zero Shot Time Series Forecasters.** *Nate Gruver(NYU), Marc Finzi(CMU), Shikai Qiu(NYU), and Andrew G. Wilson(NYU).* [link](https://arxiv.org/abs/2310.07820) [code](https://github.com/ngruver/llmtime) :star:663
- **A decoder-only foundation model for time-series forecasting.** *Das, Abhimanyu(Google Research), Weihao Kong, Rajat Sen, and Yichen Zhou.* [link](https://arxiv.org/pdf/2310.10688) [code](https://github.com/google-research/timesfm) :star:3.6k

### 2024

- **Tiny Time Mixers (TTMs): Fast Pre-trained Models for Enhanced Zero/Few-Shot Forecasting of Multivariate Time Series.** *Ekambaram, Vijay(IBM Granite), Arindam Jati, Nam H. Nguyen, Pankaj Dayama, Chandra Reddy, Wesley M. Gifford, and Jayant Kalagnanam. * [link](https://arxiv.org/abs/2401.03955) [code](https://github.com/ibm-granite/granite-tsfm) :star:348
- **Lag-Llama: Towards Foundation Models for Probabilistic Time Series Forecasting.** *Rasul, Kashif, Arjun Ashok, Andrew Robert Williams, Hena Ghonia, Rishika Bhagwatkar, Arian Khorasani, Mohammad Javad Darvishi Bayazi et al.* [link](https://time-series-foundation-models.github.io/lag-llama.pdf) [code](https://github.com/time-series-foundation-models/lag-llama/) :star:1.2k
- **Unified Training of Universal Time Series Forecasting Transformers.** *Woo, Gerald(Salesforce AI Research), Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, and Doyen Sahoo.* [link](https://arxiv.org/pdf/2402.02592) [code](https://github.com/SalesforceAIResearch/uni2ts) :star:788
- **Chronos: Learning the Language of Time Series.** *Das, Abhimanyu(Google Research), Weihao Kong, Rajat Sen, and Yichen Zhou.* [link](https://arxiv.org/abs/2403.07815) [code](https://github.com/amazon-science/chronos-forecasting) :star:2.4k


## Dataset